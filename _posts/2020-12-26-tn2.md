---
title : "Tensor network (2)"
excerpt: "A practical introduction to tensor networks by Roman Orus 2"
categories :
    - statistical physics
    - summary
author_profile : true
toc : true
use_math : true
---

## Tensor network theory

이번에는 조금 수학적인 내용을 소개할 것이다. 여기서 우리는 TN 상태를 정의하고, 이를 TN diagram으로 표현하는 방법을 알아볼 것이다. 

### Tensors, tensor networks, and tensor network diagrams

텐서란, 복소수의 다차원 배열이다. 텐서의 rank는 첨자의 갯수를 나타낸다. 즉, rank-$0$ tensor는 스칼라$(x)$, rank-$1$ tensor는 벡터$(v_{\alpha})$, 그리고 rank-$2$ tensor는 행렬$(A_{\alpha\beta})$이라고 이해할 수 있다.

index contraction이라는 것이 있는데, 이는 반복되는 첨자에 대해서 모든 수를 다 더하는 것이다. 예를 들어 다음과 같은 행렬곱을 생각하자.

$$C_{\alpha\gamma} = \sum_{\beta =1}^D A_{\alpha\beta} B_{\beta\gamma} \tag{1}$$

이것은 첨자 $\beta$에 대한 contraction이다. 그리고 다음과 같이 이보다 더 복잡한 형태의 contraction도 생각할 수 있다.

$$F_{\gamma\omega\rho\sigma} = \sum_{\alpha, \beta, \delta, \mu, \nu=1}^D A_{\alpha\beta\delta\sigma}B_{\beta\gamma\mu}C_{\delta\nu\mu\omega}E_{\mu\rho\alpha} \tag{2}$$

단순함을 위해 contracted indices들이 모두 $D$개의 다른 값을 가진다고 했지만, 굳이 그럴 필요는 없다. 이 예시들로 알 수 있듯이, 텐서들 사이의 contraction은 새로운 텐서를 만들어낸다. 그리고 contract 되지 않은 첨자들을 open indices라고 부른다.

TN(Tensor network)은 모든 첨자가 특정한 방식으로 contract 되어있는 텐서들의 집합이다. $(1)$과 $(2)$ 두가지가 TN의 간단한 예시다. 일반적으로 TN contraction을 거쳤을 때 open indices의 갯수가 $0$개면 스칼라, $0$개가 아니면 그 결과는 또 다른 텐서이다. 이 지점에서 TN을 도형으로 나타내는 방법인 TN diagram을 알아보도록 하자.

![ex_screenshot](/assets/images/TN/fig5.jpg)

위 도표에서 텐서는 도형으로, 첨자는 도형에서 뻗어나오는 선으로 나타내어진다. 두 텐서 사이에 서로 연결되어 있는 선분은 텐서 사이의 contraction을 나타내고, 연결되어있지 않은 선분은 텐서의 open indices를 나타낸다.

TN diagram을 사용하면 계산을 더 쉽게 다룰수가 있다. 예를 들어, 몇 개의 텐서 사이의 contraction은 다음 그림과 같이 표현된다.

![ex_screenshot](/assets/images/TN/fig6-7.jpg)

그림 7을 보면 알 수 있듯이, cyclic property를 표현하는 복잡한 방정식 대신에 TN diagram을 사용하면 시스템이 가진 특징을 쉽게 유추할 수 있다는 장점이 있다. 양자장론에서의 파인만 도표와 비교해봐도 긴 방정식을 도표로 시각화시키는 것이 더 직관적이라는 것을 알 수 있다. 따라서, 지금부터 우리는 TN을 나타내기 위해 도형을 적극적으로 사용할 것이다.

본격적으로 TN diagram을 사용하기 전에 강조해야 할 중요한 성질이 하나 있다. TN contraction의 결과를 얻기 위해 수행해야 할 연산의 수는 어떤 순서로 contraction이 일어나는가에 크게 의존한다. 다음 그림을 보자.

![ex_screenshot](/assets/images/TN/fig8.jpg)

각각의 경우는 모두 같은 TN contraction에 해당한다. 하지만 어떤 경우에는 연산의 수가 $O(D^5)$지만 어떤 경우에는 $O(D^4)$이다. 우리가 TN을 다룰 때는 매우 많은 수의 contraction을 수행해야 하기 때문에, 가능한 효율적으로 계산을 수행하는 것이 좋다. 이것을 위해 가장 최적화된 contraction의 순서를 찾고, 그에 따라 코드를 작성하는 것이 매우 중요하다. 실용적인 단계에서는 간단한 검사를 통해 최적화 시킬 수 있지만, 수학적으로는 매우 어려운 문제이다. 

### Breaking the wave-function into small piece

















## Reference

[1] A practical introduction to tensor networks, Roman Orus, arXiv:1306.2164
