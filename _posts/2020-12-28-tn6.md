---
title : "Tensor network (6) - Determining the tensors: finding ground states"
excerpt: "A practical introduction to tensor networks by Roman Orus"
categories :
    - computational methods

author_profile : true
toc : true
use_math : true
---

MPS와 PEPS는 다체계에서의 국소적 해밀토니안의 바닥상태나 low-energy excitation을 근사적으로 표현할 수 있기 때문에 매우 유용하고, 이전에 설명했듯 TN이 다체계 힐베르트 공간 내에서 관련있는 작은 공간을 다루기 때문에 TN을 사용하는 것은 좋은 아이디어이다. 다른 말로 표현하면 MPS와 PEPS는 entanglement entropy의 area-law에 대한 구조를 이미 가지고 있다. 따라서 많은 local system의 바닥상태에 대한 얽힘을 적절하게 표현할 수 있다. 일반적으로, MPS와 PEPS가 gapped local Hamiltonian의 유한한 온도 범위에서의 특징을 잘 근사한다고 알려져있다. 따라서 MPS와 PEPS를 ansatz로 사용하는 것은 그리 놀라운 일이 아니다.

그러나 여전히 남아있는 질문이 있다. 어떻게 이들 텐서의 계수를 집어넣을것인가? 이에 대한 답은 보고자 하는 시스템의 유형이나 보고자 하는 상태에 따라 다르다. 지금은 TN으로 바닥상태를 찾을 때 가장 자주 활용되는 variational optimization, imaginary time evolution에 대해 소개할 것이다. 이 것은 다른 테크닉을 배우는데 좋은 시작점이 될 것이다.

## Variational optimization

해밀토니안 $H$가 주어져있다고 할 때, variational principle은 어떤 상태 $\vert \Psi \rangle$에 대해 다음이 항상 성립한다는 것을 알려준다.

$$\frac{\langle \Psi \vert H \vert \Psi \rangle}{\langle \Psi \vert \Psi \rangle} \ge E_0$$

$E_0$는 $H$의 바닥상태의 에너지이다. 그러므로 만약 $\vert \Psi \rangle$가 bond dimension $D$를 가지는 (MPS나 PEPS와 같은) TN이라면 우리는 항상 다음 기댓값을 최소화시킴으로써 바닥상태를 근사할 수 있을 것이다.

$$\min_{\vert\Psi\rangle\in\text{family}} (\langle\Psi\vert H\vert\Psi\rangle - \lambda\langle\Psi\vert\Psi\rangle)$$

이 때, $\lambda$는 $\vert\Psi\rangle$이 항상 $1$의 norm을 가지도록 하는 Lagrange multiplier이다.

이상적으로, 이 최소화는 TN의 모든 텐서의 계수에 대해 동시에 진행될 수 있다. 그러나 이는 구현하기도 힘들고 그다지 효율적이지도 않다. 이 방법 대신에, 한 번에 하나의 텐서만 최적화하는 방법을 사용한다. 즉, 다른 텐서들은 고정시키고 하나의 텐서만 최소화시키는 방식을 모든 텐서에 대해 적용하는 것이다. 실제 계산에서는 원하는 기댓값이 얻어질 때 까지 모든 텐서에 대해 이 과정을 여러번 수행하게 된다.

하나의 텐서를 가지고 최소화시키는 방법은 다음과 같다. 먼저 하나의 텐서 $A$를 제외한 다른 모든 텐서들을 고정한다. 이제 텐서 $A$의 계수들이 우리의 variational parameters다. 이는 다음과 같이 쓸 수 있다.

$$\min_{A}(\langle\Psi\vert H\vert\Psi\rangle - \lambda\langle\Psi\vert\Psi\rangle ) = \min_{A} \left(\vec{A}^\dagger \mathcal{H}_{\text{eff}}\vec{A} - \lambda\vec{A}^\dagger \mathcal{N}\vec{A} \right)$$

![ex_screenshot](/assets/images/TN/fig43.jpg)

위의 방정식에서 $\vec{A}$는 단지 $A$의 계수들을 벡터로 재배열한 것을 나타내고(그림 43.a), $\mathcal{H}_{\text{eff}}$는 유효 해밀토니안, 그리고 $\mathcal{N}$은 normalization matrix다. 실제로는 $\mathcal{H}_{\text{eff}}$와 $\mathcal{N}$이 텐서 $A$와 $A^*$가 없을 때의 $\langle\Psi\vert H \vert\Psi\rangle$와 $\langle\Psi\vert\Psi\rangle$에 해당한다. 즉, 텐서 $A$와 $A^*$의 environment를 행렬 형태로 쓴 것과 마찬가지다(그림 43.b). 이제, 최소화 과정은 다음과 같이 쓰여진다.

$$\frac{\partial}{\partial\vec{A}^\dagger}\left(\vec{A}^\dagger \mathcal{H}_{\text{eff}} \vec{A} - \lambda \vec{A}^\dagger \mathcal{N}\vec{A} \right) =0$$

이는 다음과 같은 일반화된 고윳값 문제로 나타내어진다.

$$\mathcal{H}_{\text{eff}}\vec{A} = \lambda\mathcal{N}\vec{A} $$

$\mathcal{H}_{\text{eff}}$과 $\mathcal{N}$을 알고 나면 이 일반화된 고윳값 문제는 선형대수 라이브러리를 사용해서 쉽게 구할 수 있다. 그리고 $\mathcal{H}_{\text{eff}}$과 $\mathcal{N}$을 계산하기 위해서는 지난 절에 설명했듯이 effective environment를 구하는 방법을 사용하면 된다.

이제 몇가지를 언급할 차례이다. 먼저, 우리가 열린 경계를 가진 MPS에 대해 이 optimization을 시행하면, 우리는 단순히 MPS를 사용해서 Density Matrix Renormalization Group(DMRG) method를 적용하는 것이다. MPS를 사용할 때 주기적인 경계를 가지는 경우로 확장하기가 용이하기 때문에 이는 매력적인 선택지이다. 두번째로, variational technique는 iMPS에 대해서도 성공적우로 적용할 수 있다. 심지어 finite PEPS에도 적용할 수 있다. 세번째로, 이 알고리즘에서 normalization matrix $\mathcal{N}$에 의존하는 stability issue가 있다. 이를 잠시 후에 살펴볼 것이다.

## Imaginary time evolution

Imaginary time evolution은 관심있는 해밀토니안의 초기상태에서 허수 시간에 대한 time evolution을 보는 방법이다. 매우 긴 시간에 대해 최종적 양자상태는 바닥상태 $\vert E_0 \rangle$에 머무른다.

$$\vert E_0 \rangle = \lim_{\tau\rightarrow\infty} \frac{e^{-\tau H}\vert\Psi(0)\rangle}{\sqrt{\vert\langle\Psi(\tau)\vert\Psi(\tau)\rangle\vert}}, \qquad \vert\Psi(\tau)\rangle = e^{-\tau H} \vert \Psi(0)\rangle$$

초기 상태 $\vert \Psi(0) \rangle$는 바닥상태와의 overlap이 $0$이 아니라고 가정한다. 이제 주어진 해밀토니안에 대해 관련된 TN의 time evolution을 구현하는 방법을 찾아야 한다. 이는 time evolution operator를 $\e^{-\tau H} = \left( e^{-\delta\tau H} \right)^m$으로 나누면 해결된다. 다음으로 해밀토니안을 다음과 같이 two-body n.n term으로 나누어 적는다. 

$$H = \sum_{\langle ij\rangle} h_{i,j}$$

그리고 time evolution operator를 $\delta\tau$의 1차항까지 전개하면 다음을 얻는다.

$$e^{-\delta\tau H} = \prod_{\langle i,j \rangle} e^{-\delta\tau h_{ij}} + O(\delta\tau^2)$$

이것을 first-order Suzuki-Trotter expansion이라고 부른다. 오차를 줄여야 할 필요를 느낀다면 더 높은 항까지 전개해서 쓰면 된다. 위의 식에서 곱해진 각각의 항을 two-body gate라 부르는데, 이는 quantum circuit을 연상시키는 용어이다. 즉, two-body gate $g_{ij}$는 다음과 같이 주어진다.

$$g_{ij} \equiv e^{-\delta\tau h_{ij}}$$

이와 같이 위의 모든 과정은 imaginary-time evolution operator를 $m$번 반복되는 $U(\delta\tau) \equiv \prod_{\langle i,j \rangle} g_{ij}$로 근사하는 것을 의미한다. 이것은 원래 문제를 매우 단순화시키지만, 주어진 TN state에 대해 이 연산자를 어떻게 적용해야 할지 알아야한다. 운좋게도 그림 44에서 보듯 이를 달성하는 많은 방법이 존재한다. 

![ex_screenshot](/assets/images/TN/fig44.jpg)

이는 크게 다음 두 과정으로 진행된다.

**(1) Infisitesimal evolution:** bond dimension $D$를 가지는 주어진 TN state $\vert \Psi \rangle$에 연산자 $U(\delta\tau)$를 적용해서 bond dimension $D'\ge D$를 가지는 새로운 TN state $\vert \tilde{\Psi} \rangle=U(\delta\tau)\vert\Psi\rangle$를 얻는다.

**(2) Truncation:** $\vert \tilde{\Psi}\rangle$을 bond dimension $D$를 가지는 새로운 TN state $\vert \Psi' \rangle$로 근사한다.

일반적으로, 시스템의 차원과 원하는 정확도에 따라 이를 구현하는 매우 많은 방법이 존재한다. 예상했던 대로, 더 정확한 알고리즘은 더 높은 계산 복잡도와 효율을 가지지만 덜 정확한 알고리즘은 효율적이긴 하지만 시스템의 얽힘이 큰 경우에는 부정확한 결과를 준다. 예로 1차원에서 과정 (1)을 구현하는 방법이 그림 45에 제시되어 있다.

![ex_screenshot](/assets/images/TN/fig45.jpg)

게다가 과정 (2)는 다음과 같은 squared distance error을 최소화함으로써 구현할 수 있다.

$$err = \left\vert \left\vert \vert \Psi'\rangle - \vert\tilde{\Psi}\rangle \right\vert \right\vert$$

그리고 이 최소화는 다시 텐서별로 진행된다. 텐서 $A$에 대한 최소화는 다음과 같다.

$$\min_{A}\left\vert \left\vert \vert \Psi'\rangle - \vert\tilde{\Psi}\rangle \right\vert \right\vert = \min_{A} \left( \vec{A}^\dagger \mathcal{N} \vec{A} - \vec{A}^\dagger \vec{\mathcal{B}} - \vec{\mathcal{B}}^\dagger \vec{A} + \mathcal{C} \right)$$

이 때 $\vec{\mathcal{B}}$는 $A$의 environment에 해당하는 벡터이고, $\mathcal{C}=\langle\tilde{\Psi}\vert\tilde{\Psi}\rangle$는 스칼라이다. 다시 $\mathcal{N}$, $\vec{\mathcal{B}}$ 그리고 $\mathcal{C}$는 지난 장에서 설명했던 것과 같은 방법으로 계산한다. 이 최소화 과정은

$$\frac{\partial}{\partial\vec{A}^\dagger}\left( \vec{A}^\dagger \mathcal{N} \vec{A} - \vec{A}^\dagger \vec{\mathcal{B}} - \vec{\mathcal{B}}^\dagger \vec{A} + \mathcal{C} \right) = 0$$

로 표현되고, 이는 최종적으로 다음과 같은 형태를 준다.

$$\mathcal{N}\vec{A}=\vec{\mathcal{B}}$$

이 것은 단순히 연립방정식을 통해 $A$의 계수를 구하는 것이다. 끝으로 $\mathcal{N}$과 $\vec{\mathcal{B}}$을 알면, 위의 방정식을 풀면 된다. 



















## Reference

[1] A practical introduction to tensor networks, Roman Orus, arXiv:1306.2164
